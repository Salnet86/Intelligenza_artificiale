{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea609fbd-b481-4d61-a442-d070f1c3716b",
   "metadata": {},
   "source": [
    "<h1 style='color:lime;'>INTELIGENZA ARTIFICIALE AI\n",
    "TECNICA DI ONE HOTE\n",
    "RETE FEEDFORWARDRETE NEURALE RNN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395043a7-a7b1-4c51-891b-10f30f40f580",
   "metadata": {},
   "source": [
    "<p style='color:blue;'>\n",
    "Stagisti Corso Addetto alla logistica automatica ambito IOT FORIT GROUP , POLITECNICO RIZZO AGENZIA GI GROUP</p>\n",
    "\n",
    "Anno 09/05/2022 – 27/05/2022\n",
    "<br>\n",
    "Autori del codice\n",
    "<br>\n",
    "Valentina ONE HOTE FEEDFAWARD DNN \n",
    "<br>\n",
    "Salvatore ONE HOTE FEEDFAWARD DNN \n",
    "<br>\n",
    "Paola RNN \n",
    "<br>\n",
    "Fabrizzio RNN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dcccf9e-0caf-4c0f-9edf-ff37ec24109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Inserisci una frase:  ciao\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stato RNN finale (primi 10 neuroni): [0.04077306869552851, 0.21562149754035737, 0.030810700235421273, 0.12590356514686407, 0.18665031855702904, 0.1767757630646206, 0.08867941700374618, 0.05013819151375852, 0.18922741648323083, 0.043225596459888466] ...\n",
      "Probabilità softmax: [0.29083468188957706, 0.35708226456237274, 0.3520830535480502]\n",
      "Parola di risposta: come\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# Funzioni\n",
    "# -----------------------------\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_deriv_from_activation(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def softmax(x):\n",
    "    m = max(x)  # stabilità numerica\n",
    "    e_x = [math.exp(i - m) for i in x]\n",
    "    somma = sum(e_x)\n",
    "    return [i / somma for i in e_x]\n",
    "\n",
    "# -----------------------------\n",
    "# Input utente\n",
    "# -----------------------------\n",
    "input_utente = input(\"Inserisci una frase: \").lower()\n",
    "parole_input = input_utente.split()\n",
    "\n",
    "dati = {\n",
    "    \"tag\": \"salutare\",\n",
    "    \"partens\": ['ciao', 'come', 'stai', 'ciao'],\n",
    "    \"response\": ['bene, grazie, tutto bene']\n",
    "}\n",
    "\n",
    "# Vocabolario unico\n",
    "vocabolario = []\n",
    "for parola in dati['partens']:\n",
    "    if parola not in vocabolario:\n",
    "        vocabolario.append(parola)\n",
    "\n",
    "# One-hot encoding\n",
    "one_hot = [1 if parola in parole_input else 0 for parola in vocabolario]\n",
    "\n",
    "# -----------------------------\n",
    "# Parametri rete\n",
    "# -----------------------------\n",
    "epoche = 500\n",
    "rate = 0.8\n",
    "neuroni = 100  # neuroni dello stato nascosto\n",
    "\n",
    "# Target (esempio)\n",
    "target = [0] * len(vocabolario)\n",
    "target[0] = 1\n",
    "\n",
    "# Pesi feedforward\n",
    "pesi = [[random.uniform(0, 0.1) for _ in range(neuroni)] for _ in range(len(vocabolario))]\n",
    "bias = [random.uniform(0, 0.2) for _ in range(neuroni)]\n",
    "\n",
    "output_per_parola = {parola: [] for parola in vocabolario}\n",
    "Out_RNN = []\n",
    "\n",
    "# -----------------------------\n",
    "# Addestramento feedforward\n",
    "# -----------------------------\n",
    "for iterate_epoche in range(epoche):\n",
    "    for vocabolo in range(len(vocabolario)):\n",
    "        one = one_hot[vocabolo]\n",
    "        for n in range(neuroni):\n",
    "            z1 = pesi[vocabolo][n] * one + bias[n]\n",
    "            a1 = sigmoid(z1)\n",
    "            errore = a1 - target[vocabolo]\n",
    "            derivata_sigmoid = sigmoid_deriv_from_activation(a1)\n",
    "\n",
    "            pesi[vocabolo][n] -= rate * errore * derivata_sigmoid * one\n",
    "            bias[n] -= rate * errore * derivata_sigmoid\n",
    "\n",
    "            output_per_parola[vocabolario[vocabolo]].append(a1)\n",
    "\n",
    "        Out_RNN.append(sum(output_per_parola[vocabolario[vocabolo]]) / len(output_per_parola[vocabolario[vocabolo]]))\n",
    "\n",
    "# -----------------------------\n",
    "# Inizializzazione RNN\n",
    "# -----------------------------\n",
    "WT = [random.uniform(0, 0.2) for _ in range(neuroni)]\n",
    "WH = [random.uniform(0, 0.2) for _ in range(neuroni)]\n",
    "bias_t = [random.uniform(0, 0.2) for _ in range(neuroni)]\n",
    "hidden_state = [0.0 for _ in range(neuroni)]\n",
    "\n",
    "# Layer di output che mappa hidden_state al vocabolario\n",
    "W_out = [[random.uniform(0, 0.2) for _ in range(neuroni)] for _ in range(len(vocabolario))]\n",
    "b_out = [random.uniform(0, 0.2) for _ in range(len(vocabolario))]\n",
    "\n",
    "# -----------------------------\n",
    "# Funzione RNN aggiornata\n",
    "# -----------------------------\n",
    "def RNN(Out_RNN, hidden_state):\n",
    "    nuovo_hidden = hidden_state[:]  # copia dello stato iniziale\n",
    "    for out in Out_RNN:             # aggiorniamo parola per parola\n",
    "        for n in range(neuroni):\n",
    "            nuovo_hidden[n] = math.tanh(WT[n] * out + WH[n] * nuovo_hidden[n] + bias_t[n])\n",
    "    return nuovo_hidden\n",
    "\n",
    "# -----------------------------\n",
    "# Aggiornamento RNN\n",
    "# -----------------------------\n",
    "hidden_state = RNN(Out_RNN, hidden_state)\n",
    "print(\"Stato RNN finale (primi 10 neuroni):\", hidden_state[:10], \"...\")\n",
    "\n",
    "# -----------------------------\n",
    "# Calcolo output sul vocabolario\n",
    "# -----------------------------\n",
    "logits = []\n",
    "for j in range(len(vocabolario)):\n",
    "    somma = 0.0\n",
    "    for i in range(neuroni):\n",
    "        somma += hidden_state[i] * W_out[j][i]\n",
    "    logits.append(somma + b_out[j])\n",
    "\n",
    "# Softmax per probabilità\n",
    "output_prob = softmax(logits)\n",
    "print(\"Probabilità softmax:\", output_prob)\n",
    "\n",
    "# Scelta parola più probabile\n",
    "parola_scelta = vocabolario[output_prob.index(max(output_prob))]\n",
    "print(\"Parola di risposta:\", parola_scelta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705b5fb-f9ba-4630-85b4-012f1fecee73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690a055-c8db-4616-b560-7565008b64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input utente: \"ciao come stai\"\n",
    "           │\n",
    "           ▼\n",
    "  Split in parole → [\"ciao\", \"come\", \"stai\"]\n",
    "           │\n",
    "           ▼\n",
    "One-hot encoding per ogni parola\n",
    "           │\n",
    "           ▼\n",
    "Feedforward (per ogni parola)\n",
    " ┌─────────────────────────────────────────┐\n",
    " │ z = peso*one + bias                      │\n",
    " │ a = sigmoid(z)  ← attivazione neuroni   │\n",
    " └─────────────────────────────────────────┘\n",
    "           │\n",
    "           ▼\n",
    "Out_RNN = valori attivazione neuroni\n",
    "           │\n",
    "           ▼\n",
    "RNN (stato nascosto)\n",
    " ┌─────────────────────────────────────────┐\n",
    " │ hidden_state[n] = tanh(WT*n + WH*hidden_state[n] + bias_t) │\n",
    " │ per ogni parola e neurone                                   │\n",
    " └─────────────────────────────────────────┘\n",
    "           │\n",
    "           ▼\n",
    "Layer output (vocabolario)\n",
    " ┌─────────────────────────────────────────┐\n",
    " │ logits[j] = Σ(hidden_state[i]*W_out[j][i]) + b_out[j]      │\n",
    " │ softmax(logits) → probabilità per ogni parola               │\n",
    " └─────────────────────────────────────────┘\n",
    "           │\n",
    "           ▼\n",
    "Scelta parola di risposta\n",
    " ┌─────────────────────────────────────────┐\n",
    " │ 1️⃣ Massima probabilità → parola scelto │\n",
    " │ 2️⃣ Random pesata (opzionale)           │\n",
    " └─────────────────────────────────────────┘\n",
    "           │\n",
    "           ▼\n",
    "Output chatbot: parola di risposta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efbb417-dbbe-484b-bbcc-295223cba4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb0c8793-d10b-4fe8-82fa-c247cf337b8c",
   "metadata": {},
   "source": [
    "1. Librerie Il programma utilizza librerie per i calcoli matematici e per generare numeri casuali. I numeri casuali servono a inizializzare i pesi e i bias della rete neurale, mentre le funzioni matematiche servono per calcolare le attivazioni dei neuroni e le funzioni di attivazione come sigmoid, tanh e softmax.\n",
    "\n",
    "Funzioni di attivazione\n",
    "La sigmoid trasforma un numero qualsiasi in un valore tra 0 e 1, utile per decidere quanto un neurone “si attiva”.\n",
    "\n",
    "La derivata della sigmoid serve per aggiornare i pesi della rete durante l’addestramento.\n",
    "\n",
    "La softmax trasforma un insieme di valori in probabilità, così che le uscite possano essere interpretate come probabilità di ciascuna parola.\n",
    "\n",
    "Input dell’utente\n",
    "L’utente scrive una frase. La frase viene convertita in minuscolo e divisa in singole parole, per poter confrontare ogni parola con quelle presenti nel vocabolario della rete.\n",
    "\n",
    "Dati di addestramento\n",
    "Il programma ha un insieme di dati di esempio, con alcune parole e una risposta associata. Questi dati servono a insegnare alla rete a rispondere in maniera corretta. Per semplicità, ogni parola del vocabolario ha un “target” associato che indica quale parola vogliamo predire.\n",
    "\n",
    "Vocabolario\n",
    "Viene creato un elenco di parole uniche, eliminando duplicati. Questo elenco rappresenta tutte le parole che la rete conosce e può prevedere.\n",
    "\n",
    "Codifica one-hot\n",
    "Ogni parola dell’input dell’utente viene trasformata in un vettore di 0 e 1, dove 1 indica che la parola è presente nella frase e 0 indica che non lo è. Questo permette alla rete di lavorare con numeri invece che con parole.\n",
    "\n",
    "Nota: esiste anche una tecnica più avanzata chiamata word embedding, che trasforma le parole in vettori continui di dimensione inferiore. A differenza del one-hot, gli embedding catturano somiglianze tra parole, quindi parole simili avranno vettori simili nello spazio. Una rete neurale può imparare automaticamente questi embedding a partire dai vettori one-hot.\n",
    "\n",
    "Parametri della rete\n",
    "Si definiscono:\n",
    "\n",
    "Il numero di epoche, cioè quante volte addestrare la rete.\n",
    "\n",
    "Il learning rate, che indica quanto i pesi vengono aggiornati a ogni passo.\n",
    "\n",
    "Il numero di neuroni dello stato nascosto della RNN, cioè quanti neuroni memorizzano l’informazione durante l’elaborazione della frase.\n",
    "\n",
    "Si inizializzano anche i pesi e i bias con valori casuali.\n",
    "\n",
    "Addestramento feedforward\n",
    "La rete calcola l’attivazione dei neuroni in base all’input, confronta il risultato con il target desiderato, calcola l’errore e aggiorna i pesi per ridurre quell’errore. Questo processo viene ripetuto molte volte per tutti i neuroni e tutte le parole del vocabolario. Alla fine, per ogni parola viene calcolata un’attivazione media da passare alla RNN.\n",
    "\n",
    "Inizializzazione della RNN\n",
    "La RNN ha dei pesi e dei bias specifici per aggiornare il suo stato nascosto, che inizialmente è azzerato. C’è anche un layer di output che serve a trasformare lo stato nascosto in un risultato che rappresenta la probabilità delle parole del vocabolario.\n",
    "\n",
    "Funzionamento della RNN\n",
    "La RNN aggiorna lo stato nascosto combinando le informazioni dell’input corrente (feedforward) e dello stato precedente. La funzione di attivazione usata è tanh, che restituisce valori tra -1 e 1. Lo stato nascosto finale contiene la “memoria” della frase inserita dall’utente.\n",
    "\n",
    "Aggiornamento dello stato nascosto\n",
    "Lo stato nascosto della RNN viene calcolato e rappresenta la conoscenza della frase elaborata dalla rete. Viene mostrato un estratto dei primi neuroni per visualizzare il contenuto dello stato.\n",
    "\n",
    "Calcolo dell’output\n",
    "Lo stato nascosto viene combinato con i pesi e i bias del layer di output per ottenere un insieme di valori chiamati “logit”. Questi valori vengono poi trasformati in probabilità tramite softmax. La parola con la probabilità più alta viene scelta come risposta della rete.\n",
    "\n",
    "Sintesi del flusso\n",
    "Il programma:\n",
    "\n",
    "Riceve una frase dall’utente e la converte in numeri (one-hot o embedding).\n",
    "\n",
    "Passa i numeri attraverso una rete feedforward per ottenere attivazioni iniziali.\n",
    "\n",
    "Aggiorna lo stato nascosto di una RNN che memorizza le informazioni della frase.\n",
    "\n",
    "Usa lo stato nascosto per calcolare la probabilità di ciascuna parola.\n",
    "\n",
    "Restituisce la parola più probabile come risposta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbe487-0de3-4bc0-8b68-b2d035c33a9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3bf82f-64fe-4cce-9610-8b1adfb7a57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport random\\nimport math\\n\\n# -----------------------------\\n# Funzioni di attivazione\\n# -----------------------------\\n\\ndef sigmoid(x):\\n    \"\"\"Funzione sigmoid: trasforma x in un valore tra 0 e 1\"\"\"\\n    return 1 / (1 + math.exp(-x))\\n\\ndef sigmoid_deriv_from_activation(a):\\n    \"\"\"Derivata della sigmoid calcolata dall\\'attivazione\"\"\"\\n    return a * (1 - a)\\n\\ndef softmax(x):\\n    \"\"\"\\n    Softmax: converte una lista di valori (logits) in probabilità\\n    - Sottrae il massimo per stabilità numerica\\n    \"\"\"\\n    m = max(x)\\n    e_x = [math.exp(i - m) for i in x]\\n    somma = sum(e_x)\\n    return [i / somma for i in e_x]\\n\\n# -----------------------------\\n# Input utente e vocabolario\\n# -----------------------------\\n\\ninput_utente = input(\"Inserisci una frase: \").lower()  # prendi input e rendilo minuscolo\\nparole_input = input_utente.split()                    # dividi in parole\\n\\n# Dati di esempio\\ndati = {\\n    \"tag\": \"salutare\",\\n    \"partens\": [\\'ciao\\', \\'come\\', \\'stai\\', \\'ciao\\'],\\n    \"response\": [\\'bene, grazie, tutto bene\\']\\n}\\n\\n# Costruzione vocabolario unico\\nvocabolario = []\\nfor parola in dati[\\'partens\\']:\\n    if parola not in vocabolario:\\n        vocabolario.append(parola)\\n\\n# One-hot encoding dell\\'input\\none_hot = [1 if parola in parole_input else 0 for parola in vocabolario]\\n\\n# -----------------------------\\n# Parametri rete\\n# -----------------------------\\n\\nepoche = 500       # numero di epoche di addestramento\\nrate = 0.8         # learning rate\\nneuroni = 100      # numero di neuroni nascosti\\n\\n# Target di esempio (prima parola corretta)\\ntarget = [0] * len(vocabolario)\\ntarget[0] = 1\\n\\n# -----------------------------\\n# Inizializzazione pesi feedforward\\n# -----------------------------\\n\\n# Pesi: vocabolario x neuroni\\npesi = [[random.uniform(0, 0.1) for _ in range(neuroni)] for _ in range(len(vocabolario))]\\n# Bias per neuroni nascosti\\nbias = [random.uniform(0, 0.2) for _ in range(neuroni)]\\n\\n# Dizionari per memorizzare le attivazioni\\noutput_per_parola = {parola: [] for parola in vocabolario}\\nOut_RNN = []\\n\\n# -----------------------------\\n# Addestramento feedforward (minimale)\\n# -----------------------------\\n\\nfor iterate_epoche in range(epoche):\\n    for vocabolo in range(len(vocabolario)):\\n        one = one_hot[vocabolo]  # valore 0 o 1 della parola\\n        for n in range(neuroni):\\n            # Feedforward: calcolo z e attivazione\\n            z1 = pesi[vocabolo][n] * one + bias[n]\\n            a1 = sigmoid(z1)\\n\\n            # Errore e derivata sigmoid\\n            errore = a1 - target[vocabolo]\\n            derivata_sigmoid = sigmoid_deriv_from_activation(a1)\\n\\n            # Aggiornamento pesi e bias (gradient descent)\\n            pesi[vocabolo][n] -= rate * errore * derivata_sigmoid * one\\n            bias[n] -= rate * errore * derivata_sigmoid\\n\\n            # Memorizza attivazione neurone\\n            output_per_parola[vocabolario[vocabolo]].append(a1)\\n\\n        # Media delle attivazioni per la RNN\\n        Out_RNN.append(sum(output_per_parola[vocabolario[vocabolo]]) / len(output_per_parola[vocabolario[vocabolo]]))\\n\\n# -----------------------------\\n# Inizializzazione RNN\\n# -----------------------------\\n\\n# Pesi input→hidden e hidden→hidden\\nWT = [random.uniform(0, 0.2) for _ in range(neuroni)]\\nWH = [random.uniform(0, 0.2) for _ in range(neuroni)]\\n# Bias della RNN\\nbias_t = [random.uniform(0, 0.2) for _ in range(neuroni)]\\n# Stato nascosto iniziale\\nhidden_state = [0.0 for _ in range(neuroni)]\\n\\n# Layer di output che mappa hidden_state al vocabolario\\nW_out = [[random.uniform(0, 0.2) for _ in range(neuroni)] for _ in range(len(vocabolario))]\\nb_out = [random.uniform(0, 0.2) for _ in range(len(vocabolario))]\\n\\n# -----------------------------\\n# Funzione RNN aggiornata\\n# -----------------------------\\n\\ndef RNN(Out_RNN, hidden_state):\\n    \"\"\"\\n    Aggiorna lo stato nascosto neurone per neurone e parola per parola.\\n    - Out_RNN: lista delle attivazioni del feedforward\\n    - hidden_state: stato nascosto iniziale\\n    \"\"\"\\n    nuovo_hidden = hidden_state[:]  # copia dello stato iniziale\\n    for out in Out_RNN:             # aggiorna parola per parola\\n        for n in range(neuroni):\\n            nuovo_hidden[n] = math.tanh(WT[n] * out + WH[n] * nuovo_hidden[n] + bias_t[n])\\n    return nuovo_hidden\\n\\n# -----------------------------\\n# Aggiornamento stato RNN\\n# -----------------------------\\n\\nhidden_state = RNN(Out_RNN, hidden_state)\\nprint(\"Stato RNN finale (primi 10 neuroni):\", hidden_state[:10], \"...\")\\n\\n# -----------------------------\\n# Calcolo output vocabolario\\n# -----------------------------\\n\\nlogits = []\\nfor j in range(len(vocabolario)):\\n    somma = 0.0\\n    for i in range(neuroni):\\n        somma += hidden_state[i] * W_out[j][i]\\n    logits.append(somma + b_out[j])\\n\\n# Softmax → probabilità\\noutput_prob = softmax(logits)\\nprint(\"Probabilità softmax:\", output_prob)\\n\\n# -----------------------------\\n# Scelta parola di risposta\\n# -----------------------------\\n\\n# Opzione 1: parola con probabilità massima\\nparola_scelta = vocabolario[output_prob.index(max(output_prob))]\\nprint(\"Parola di risposta:\", parola_scelta)\\n\\n# -----------------------------\\n# Opzione 2: scelta random pesata (più naturale)\\n# -----------------------------\\ndef scegli_parola(prob, voc):\\n    r = random.random()  # numero casuale tra 0 e 1\\n    cumulativa = 0\\n    for p, w in zip(prob, voc):\\n        cumulativa += p\\n        if r < cumulativa:\\n            return w\\n    return voc[-1]  # fallback\\n\\n# parola_scelta = scegli_parola(output_prob, vocabolario)\\n# print(\"Parola di risposta (random pesata):\", parola_scelta)\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import random\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# Funzioni di attivazione\n",
    "# -----------------------------\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Funzione sigmoid: trasforma x in un valore tra 0 e 1\"\"\"\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_deriv_from_activation(a):\n",
    "    \"\"\"Derivata della sigmoid calcolata dall'attivazione\"\"\"\n",
    "    return a * (1 - a)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax: converte una lista di valori (logits) in probabilità\n",
    "    - Sottrae il massimo per stabilità numerica\n",
    "    \"\"\"\n",
    "    m = max(x)\n",
    "    e_x = [math.exp(i - m) for i in x]\n",
    "    somma = sum(e_x)\n",
    "    return [i / somma for i in e_x]\n",
    "\n",
    "# -----------------------------\n",
    "# Input utente e vocabolario\n",
    "# -----------------------------\n",
    "\n",
    "input_utente = input(\"Inserisci una frase: \").lower()  # prendi input e rendilo minuscolo\n",
    "parole_input = input_utente.split()                    # dividi in parole\n",
    "\n",
    "# Dati di esempio\n",
    "dati = {\n",
    "    \"tag\": \"salutare\",\n",
    "    \"partens\": ['ciao', 'come', 'stai', 'ciao'],\n",
    "    \"response\": ['bene, grazie, tutto bene']\n",
    "}\n",
    "\n",
    "# Costruzione vocabolario unico\n",
    "vocabolario = []\n",
    "for parola in dati['partens']:\n",
    "    if parola not in vocabolario:\n",
    "        vocabolario.append(parola)\n",
    "\n",
    "# One-hot encoding dell'input\n",
    "one_hot = [1 if parola in parole_input else 0 for parola in vocabolario]\n",
    "\n",
    "# -----------------------------\n",
    "# Parametri rete\n",
    "# -----------------------------\n",
    "\n",
    "epoche = 500       # numero di epoche di addestramento\n",
    "rate = 0.8         # learning rate\n",
    "neuroni = 100      # numero di neuroni nascosti\n",
    "\n",
    "# Target di esempio (prima parola corretta)\n",
    "target = [0] * len(vocabolario)\n",
    "target[0] = 1\n",
    "\n",
    "# -----------------------------\n",
    "# Inizializzazione pesi feedforward\n",
    "# -----------------------------\n",
    "\n",
    "# Pesi: vocabolario x neuroni\n",
    "pesi = [[random.uniform(0, 0.1) for _ in range(neuroni)] for _ in range(len(vocabolario))]\n",
    "# Bias per neuroni nascosti\n",
    "bias = [random.uniform(0, 0.2) for _ in range(neuroni)]\n",
    "\n",
    "# Dizionari per memorizzare le attivazioni\n",
    "output_per_parola = {parola: [] for parola in vocabolario}\n",
    "Out_RNN = []\n",
    "\n",
    "# -----------------------------\n",
    "# Addestramento feedforward (minimale)\n",
    "# -----------------------------\n",
    "\n",
    "for iterate_epoche in range(epoche):\n",
    "    for vocabolo in range(len(vocabolario)):\n",
    "        one = one_hot[vocabolo]  # valore 0 o 1 della parola\n",
    "        for n in range(neuroni):\n",
    "            # Feedforward: calcolo z e attivazione\n",
    "            z1 = pesi[vocabolo][n] * one + bias[n]\n",
    "            a1 = sigmoid(z1)\n",
    "            \n",
    "            # Errore e derivata sigmoid\n",
    "            errore = a1 - target[vocabolo]\n",
    "            derivata_sigmoid = sigmoid_deriv_from_activation(a1)\n",
    "            \n",
    "            # Aggiornamento pesi e bias (gradient descent)\n",
    "            pesi[vocabolo][n] -= rate * errore * derivata_sigmoid * one\n",
    "            bias[n] -= rate * errore * derivata_sigmoid\n",
    "\n",
    "            # Memorizza attivazione neurone\n",
    "            output_per_parola[vocabolario[vocabolo]].append(a1)\n",
    "\n",
    "        # Media delle attivazioni per la RNN\n",
    "        Out_RNN.append(sum(output_per_parola[vocabolario[vocabolo]]) / len(output_per_parola[vocabolario[vocabolo]]))\n",
    "\n",
    "# -----------------------------\n",
    "# Inizializzazione RNN\n",
    "# -----------------------------\n",
    "\n",
    "# Pesi input→hidden e hidden→hidden\n",
    "WT = [random.uniform(0, 0.2) for _ in range(neuroni)]\n",
    "WH = [random.uniform(0, 0.2) for _ in range(neuroni)]\n",
    "# Bias della RNN\n",
    "bias_t = [random.uniform(0, 0.2) for _ in range(neuroni)]\n",
    "# Stato nascosto iniziale\n",
    "hidden_state = [0.0 for _ in range(neuroni)]\n",
    "\n",
    "# Layer di output che mappa hidden_state al vocabolario\n",
    "W_out = [[random.uniform(0, 0.2) for _ in range(neuroni)] for _ in range(len(vocabolario))]\n",
    "b_out = [random.uniform(0, 0.2) for _ in range(len(vocabolario))]\n",
    "\n",
    "# -----------------------------\n",
    "# Funzione RNN aggiornata\n",
    "# -----------------------------\n",
    "\n",
    "def RNN(Out_RNN, hidden_state):\n",
    "    \"\"\"\n",
    "    Aggiorna lo stato nascosto neurone per neurone e parola per parola.\n",
    "    - Out_RNN: lista delle attivazioni del feedforward\n",
    "    - hidden_state: stato nascosto iniziale\n",
    "    \"\"\"\n",
    "    nuovo_hidden = hidden_state[:]  # copia dello stato iniziale\n",
    "    for out in Out_RNN:             # aggiorna parola per parola\n",
    "        for n in range(neuroni):\n",
    "            nuovo_hidden[n] = math.tanh(WT[n] * out + WH[n] * nuovo_hidden[n] + bias_t[n])\n",
    "    return nuovo_hidden\n",
    "\n",
    "# -----------------------------\n",
    "# Aggiornamento stato RNN\n",
    "# -----------------------------\n",
    "\n",
    "hidden_state = RNN(Out_RNN, hidden_state)\n",
    "print(\"Stato RNN finale (primi 10 neuroni):\", hidden_state[:10], \"...\")\n",
    "\n",
    "# -----------------------------\n",
    "# Calcolo output vocabolario\n",
    "# -----------------------------\n",
    "\n",
    "logits = []\n",
    "for j in range(len(vocabolario)):\n",
    "    somma = 0.0\n",
    "    for i in range(neuroni):\n",
    "        somma += hidden_state[i] * W_out[j][i]\n",
    "    logits.append(somma + b_out[j])\n",
    "\n",
    "# Softmax → probabilità\n",
    "output_prob = softmax(logits)\n",
    "print(\"Probabilità softmax:\", output_prob)\n",
    "\n",
    "# -----------------------------\n",
    "# Scelta parola di risposta\n",
    "# -----------------------------\n",
    "\n",
    "# Opzione 1: parola con probabilità massima\n",
    "parola_scelta = vocabolario[output_prob.index(max(output_prob))]\n",
    "print(\"Parola di risposta:\", parola_scelta)\n",
    "\n",
    "# -----------------------------\n",
    "# Opzione 2: scelta random pesata (più naturale)\n",
    "# -----------------------------\n",
    "def scegli_parola(prob, voc):\n",
    "    r = random.random()  # numero casuale tra 0 e 1\n",
    "    cumulativa = 0\n",
    "    for p, w in zip(prob, voc):\n",
    "        cumulativa += p\n",
    "        if r < cumulativa:\n",
    "            return w\n",
    "    return voc[-1]  # fallback\n",
    "\n",
    "# parola_scelta = scegli_parola(output_prob, vocabolario)\n",
    "# print(\"Parola di risposta (random pesata):\", parola_scelta)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d039e8fb-b6c1-4a1e-9a57-7254b308cd35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
