<!doctype html>
<html lang="it">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>RNN — Formule e esempio numerico (temperature)</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body{font-family: Inter, system-ui, -apple-system, Roboto, 'Helvetica Neue', Arial;line-height:1.5;background:#fafafa;color:#111;padding:28px}
    h1,h2{color:#0b5;}
    pre{background:#222;color:#f8f8f2;padding:12px;border-radius:8px;overflow:auto}
    code{background:#eee;padding:2px 6px;border-radius:4px}
    section{margin-bottom:22px;padding:16px;background:#fff;border-radius:10px;box-shadow:0 6px 18px rgba(0,0,0,0.04)}
    table{border-collapse:collapse;width:100%}
    td,th{padding:8px;border-bottom:1px solid #eee}
  </style>
</head>
<body>
  <h1>RNN — Formule generali e esempio numerico</h1>
  <p>Documento in italiano: formule generali per una RNN semplice (1 input, 1 hidden, 1 output) + esempio con le temperature <strong>20, 30, 80, 12</strong> normalizzate a /100.</p>

  <section>
    <h2>1) Notazione e formule generali</h2>
    <p>Convezioni:</p>
    <ul>
      <li><code>x_t</code> = input al tempo <code>t</code></li>
      <li><code>h_t</code> = stato nascosto al tempo <code>t</code></li>
      <li><code>y_t</code> = output al tempo <code>t</code></li>
      <li><code>W_{xh}</code>, <code>W_{hh}</code>, <code>W_{hy}</code> = pesi</li>
      <li><code>b_h, b_y</code> = bias</li>
      <li><code>f</code> = attivazione hidden (es. <em>tanh</em>)</li>
      <li><code>g</code> = attivazione output (qui: identità/lineare)</li>
    </ul>

    <p><strong>Forward (per ogni t):</strong></p>
    <p>\[\text{net}^{(h)}_t = W_{xh}\,x_t + W_{hh}\,h_{t-1} + b_h\]</p>
    <p>\[h_t = f(\text{net}^{(h)}_t)\]</p>
    <p>\[\text{net}^{(y)}_t = W_{hy}\,h_t + b_y\]</p>
    <p>\[y_t = g(\text{net}^{(y)}_t)\]</p>

    <p><strong>Errore (MSE + output lineare):</strong></p>
    <p>\[\delta^{(out)}_t = \frac{\partial L_t}{\partial \text{net}^{(y)}_t} = y_t - target_t\]</p>

    <p><strong>Delta dello stato nascosto (BPTT, ricorsivo):</strong></p>
    <p>\[\delta^{(h)}_t = f'(\text{net}^{(h)}_t)\,\Big( \delta^{(out)}_t W_{hy} + \delta^{(h)}_{t+1} W_{hh} \Big)\]</p>

    <p><strong>Gradienti (somma sui passi t):</strong></p>
    <p>\[\frac{\partial L}{\partial W_{hy}} = \sum_{t} \delta^{(out)}_t \; h_t\]</p>
    <p>\[\frac{\partial L}{\partial W_{xh}} = \sum_{t} \delta^{(h)}_t \; x_t\]</p>
    <p>\[\frac{\partial L}{\partial W_{hh}} = \sum_{t} \delta^{(h)}_t \; h_{t-1}\]</p>
    <p>Bias:</p>
    <p>\[\frac{\partial L}{\partial b_y} = \sum_t \delta^{(out)}_t, \qquad \frac{\partial L}{\partial b_h} = \sum_t \delta^{(h)}_t\]</p>

    <p><strong>Aggiornamento (SGD):</strong> \[W \gets W - \eta \cdot \frac{\partial L}{\partial W}\]</p>
  </section>

  <section>
    <h2>2) Esempio numerico: temperature = 20, 30, 80, 12</h2>
    <p>Normalizziamo dividendo per 100: <code>x = [0.20, 0.30, 0.80, 0.12]</code></p>

    <p>Scelte iniziali (valori d'esempio):</p>
    <pre>W_xh = 0.5
W_hh = 0.1
W_hy = 0.7
b_h = 0.0
b_y = 0.0
f = tanh  (f'(z)=1-tanh^2(z))
g = identity (lineare)
learning rate η = 0.1
h_0 = 0
target_t = x_t (per semplicità)</pre>

    <h3>Forward pass (step by step)</h3>
    <pre>
-- t = 1 (x = 0.20)
net_h1 = 0.5*0.20 + 0.1*0 + 0 = 0.1
h1 = tanh(0.1) = 0.0996679946
net_y1 = 0.7*h1 = 0.0697675962
y1 = 0.0697675962

-- t = 2 (x = 0.30)
net_h2 = 0.5*0.30 + 0.1*h1 = 0.15 + 0.1*0.0996679946 = 0.15996679946
h2 = tanh(0.15996679946) = 0.15861613922541204
net_y2 = 0.7*h2 = 0.11103129745778842
y2 = 0.11103129745778842

-- t = 3 (x = 0.80)
net_h3 = 0.5*0.80 + 0.1*h2 = 0.4 + 0.1*0.1586161392 = 0.41586161392254123
h3 = tanh(0.41586161392254123) = 0.393438347905605
net_y3 = 0.7*h3 = 0.2754068435339235
y3 = 0.2754068435339235

-- t = 4 (x = 0.12)
net_h4 = 0.5*0.12 + 0.1*h3 = 0.06 + 0.1*0.393438347905605 = 0.0993438347905605
h4 = tanh(0.09934383479) = 0.09901830517427843
net_y4 = 0.7*h4 = 0.06931281362199489
y4 = 0.06931281362199489

Riassunto:
h = [0.09966799, 0.15861614, 0.39343835, 0.09901831]
y = [0.06976760, 0.11103130, 0.27540684, 0.06931281]
</pre>

    <h3>Calcolo degli errori (output deltas)</h3>
    <pre>target = x = [0.20, 0.30, 0.80, 0.12]
δ_out_t = y_t - target_t

δ_out_1 = 0.0697675962 - 0.20 = -0.1302324038
δ_out_2 = 0.1110312975 - 0.30 = -0.1889687025
δ_out_3 = 0.2754068435 - 0.80 = -0.5245931565
δ_out_4 = 0.0693128136 - 0.12 = -0.0506871864
</pre>

    <h3>Backpropagation through time — delta hidden (calcoli da t=4 a t=1)</h3>
    <pre>Formula:
δ_h_t = f'(net_h_t) * (δ_out_t * W_hy + δ_h_{t+1} * W_hh)

-- t = 4 (δ_h5 = 0)
f'(net_h4) = 1 - tanh^2(net_h4) ≈ 1 - 0.09901830517^2 ≈ 0.990
term_out = δ_out_4 * W_hy = -0.0506871864 * 0.7 = -0.03548103048
δ_h4 ≈ -0.03513315227481468

-- t = 3
f'(net_h3) = 1 - tanh^2(net_h3) ≈ 1 - 0.3934383479^2
term_out = δ_out_3 * W_hy = -0.5245931565 * 0.7 = -0.36721520955
term_from_next = δ_h4 * W_hh = -0.0351331522748 * 0.1 = -0.00351331522748
sum = -0.37072852477748
δ_h3 ≈ -0.3133420722540865

-- t = 2
term_out = δ_out_2 * W_hy = -0.1889687025 * 0.7 = -0.13227809175
term_from_next = δ_h3 * W_hh = -0.3133420722540865 * 0.1 = -0.03133420722540865
sum = -0.16361229897540865
δ_h2 ≈ -0.1594959641470257

-- t = 1
term_out = δ_out_1 * W_hy = -0.1302324038 * 0.7 = -0.09116268266
term_from_next = δ_h2 * W_hh = -0.1594959641470257 * 0.1 = -0.01594959641470257
sum = -0.10711227907470257
δ_h1 ≈ -0.10604825682173881

Riassunto δ_h ≈ [-0.10604826, -0.15949596, -0.31334207, -0.03513315]
</pre>

    <h3>Gradienti (somma su tutti i passi)</h3>
    <pre>dW_hy = Σ δ_out_t * h_t ≈ -0.2543675126
dW_xh = Σ δ_h_t * x_t ≈ -0.3239480767
dW_hh = Σ δ_h_t * h_{t-1} ≈ -0.07942048204

db_y = Σ δ_out_t ≈ -0.8944814491
db_h = Σ δ_h_t ≈ -0.6140194455
</pre>

    <h3>Aggiornamento pesi (SGD, η = 0.1)</h3>
    <pre>W_hy_new = 0.7 - 0.1 * dW_hy = 0.7 - 0.1*(-0.2543675126) = 0.7254367513
W_xh_new = 0.5 - 0.1 * dW_xh = 0.5 - 0.1*(-0.3239480767) = 0.5323948077
W_hh_new = 0.1 - 0.1 * dW_hh = 0.1 - 0.1*(-0.07942048204) = 0.1079420482

b_y_new = 0.0 - 0.1 * db_y = 0.08944814491
b_h_new = 0.0 - 0.1 * db_h = 0.06140194455
</pre>

    <p><strong>Note pratiche:</strong></p>
    <ul>
      <li>Ho normalizzato gli input (/100) per evitare saturazione di <code>tanh</code>.</li>
      <li>In un caso reale: usare batch, gradient clipping per evitare exploding-gradient e/o LSTM/GRU per problemi di vanishing-gradient.</li>
      <li>Le cifre sono arrotondate per chiarezza; i calcoli precisi usano più decimali.</li>
    </ul>
  </section>

  <section>
    <h2>3) Cosa puoi fare dopo</h2>
    <p>- Ricalcolare l'esempio senza normalizzazione per vedere la differenza.<br>- Usare output che predicono il passo successivo (next-step prediction).<br>- Ripetere con LSTM/GRU per capire le differenze nei delta.</p>
  </section>

  <footer style="margin-top:20px;font-size:13px;color:#666">Generato per SalvoNet — studia queste formule e prova a riscriverle a mano per memorizzarle.</footer>
</body>
</html>
